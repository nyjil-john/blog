{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Breast Cancer Classification with PyTorch\"\n",
        "format: html\n",
        "author: \"Nyjil Arackal\"\n",
        "date: \"2025-01-30\"\n",
        "categories: [AI, Deep Learning, PyTorch, Healthcare]\n",
        "image: \"post2_cover.jpg\"\n",
        "---\n",
        "\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Breast cancer is one of the most prevalent forms of cancer and **early detection** plays a crucial role in patient outcomes. In this project, we leverage deep learning with PyTorch to classify tumors as **benign (0)** or **malignant (1)** using the Breast Cancer Wisconsin dataset.\n",
        "\n",
        "Through data preprocessing, neural network modeling, and performance evaluation, we create a model that achieves a **98.25%** accuracy in detecting malignancies.\n",
        "\n",
        "### Dataset Overview\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "The Breast Cancer Wisconsin dataset is available in sklearn.datasets and contains **30** numerical features describing tumor properties.\n",
        "\n",
        "The target variable consists of:\\\n",
        "   -   0  :   Benign tumors (non-cancerous)\\\n",
        "   -   1  :   Malignant tumors (cancerous)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![](distribution.png)\n",
        ":::\n",
        ":::::\n",
        "\n",
        "Let's start by loading the dataset:\n"
      ],
      "id": "2dc11915"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "dataset = load_breast_cancer()\n",
        "X, y = dataset.data, dataset.target\n",
        "X.shape, y.shape # 569 samples and 30 features"
      ],
      "id": "4cdeddf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To visualize the data we convert it to Pandas DataFrame:\n"
      ],
      "id": "814d8b84"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataf = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "dataf['target'] = dataset.target"
      ],
      "id": "dfec8bd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's inspect a subset of the data:\n"
      ],
      "id": "affd0c71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataf[10:21]  # Showing samples 10 to 20\n",
        "# in this segment only the samples 19 and 20 are malignant"
      ],
      "id": "c098ea9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data preprocessing\n",
        "\n",
        "#### 1. Splitting the Dataset\n",
        "\n",
        "Splitting into training (70%), validation (20%) and test (10%) sets:\n"
      ],
      "id": "3c238fb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.7, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.33, random_state=42)"
      ],
      "id": "fa2e047f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Feature Scaling\n",
        "\n",
        "To improve model training performance, we normalize the features using MinMaxScaler:\n"
      ],
      "id": "44c31bdd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "id": "9e116706",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Converting to PyTorch Tensors\n"
      ],
      "id": "d37d79fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_valid, y_valid = torch.tensor(X_valid, dtype=torch.float32), torch.tensor(y_valid, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "X_train.shape, X_valid.shape, X_test.shape # sets: 398 training, 114 validation, 57 test"
      ],
      "id": "67dc406f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating an Accuracy Function\n"
      ],
      "id": "1d3fd8f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item() # calculates the total number of correct predictions\n",
        "  acc = (correct / len(y_pred)) * 100\n",
        "  return acc"
      ],
      "id": "3099d433",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the Neural Network\n",
        "\n",
        "Creating the Model Class:\n"
      ],
      "id": "427cc4eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from torch import nn\n",
        "torch.manual_seed(42)\n",
        "\n",
        "IN_FEATURES = X.shape[1] # 30\n",
        "OUT_FEATURES = 9 # number of neurons in the hidden layers\n",
        "OUTPUT = 1 # benign or malignant\n",
        "\n",
        "class BreastCancerModel(nn.Module):\n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_layer_stack = nn.Sequential(\n",
        "          nn.Linear(IN_FEATURES, OUT_FEATURES),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(OUT_FEATURES, OUT_FEATURES),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(OUT_FEATURES, OUTPUT)\n",
        "          )\n",
        "\n",
        "   def forward(self, x):\n",
        "        return self.linear_layer_stack(x)"
      ],
      "id": "1f565243",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up the Model:\n"
      ],
      "id": "f8b9269b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # device agnostic code\n",
        "BC_Model_V0 = BreastCancerModel().to(device) # initializing the model"
      ],
      "id": "ac2c7922",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function and Optimizer:\n"
      ],
      "id": "3f521fb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss() # Sigmoid built-in (better than: .BCELoss + Sigmoid)\n",
        "optimizer = torch.optim.Adam(params = BC_Model_V0.parameters(),\n",
        "                            lr = 0.001)"
      ],
      "id": "6e5bb803",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving data to GPU (if available):\n"
      ],
      "id": "18c7146a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# data to GPU for faster computation\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)"
      ],
      "id": "4f2b44b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n"
      ],
      "id": "2a11742b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(42)\n",
        "epochs = 500\n",
        "\n",
        "# creating lists to track results\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(epochs + 1):\n",
        "    BC_Model_V0.train()\n",
        "    y_logits = BC_Model_V0(X_train).squeeze()\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n",
        "    acc = accuracy_fn(y_true=y_train,\n",
        "                      y_pred=y_pred)\n",
        "    \n",
        "    train_losses.append(loss.item())\n",
        "    train_accuracies.append(acc) \n",
        "    \n",
        "    optimizer.zero_grad() # resetting gradients   \n",
        "    loss.backward() # calculating gradients of loss_fn\n",
        "    optimizer.step() # updating parameters\n",
        "    \n",
        "    BC_Model_V0.eval()\n",
        "    with torch.inference_mode():\n",
        "      \n",
        "      valid_logits = BC_Model_V0(X_valid).squeeze()\n",
        "      valid_pred = torch.round(torch.sigmoid(valid_logits))\n",
        "      \n",
        "      valid_loss = loss_fn(valid_logits, y_valid)\n",
        "      valid_acc = accuracy_fn(y_true=y_valid,\n",
        "                             y_pred=valid_pred)\n",
        "      \n",
        "      valid_losses.append(valid_loss.item())\n",
        "      valid_accuracies.append(valid_acc)\n",
        "      \n",
        "    if (epoch % 50 == 0 or epoch == 500):\n",
        "      print(f\"Epoch: {epoch:3n} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Validation Loss: {valid_loss:.5f}, Validation Accuracy: {valid_acc:.2f}%\")"
      ],
      "id": "86eb4f15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the Accuracy and the Loss functions.\n",
        "\n",
        "The Accuracy:\n"
      ],
      "id": "ecf0d43e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.plot(train_accuracies, label=\"Training Accuracy\", color='blue')\n",
        "plt.plot(valid_accuracies, label=\"Validation Accuracy\", color='orange')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Model Accuracy: N_9 + lr_0.001\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "dfb7dcb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Loss:\n"
      ],
      "id": "e5ffff20"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(9, 6))\n",
        "\n",
        "plt.plot(train_losses, label=\"Training Loss\", color='green')\n",
        "plt.plot(valid_losses, label=\"Validation Loss\", color='red')\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Model Loss: N_9 + lr_0.001\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "1c03b8df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the Model\n",
        "\n",
        "Let's use the final 10% of the samples to see how the model performs on unseen data:\n"
      ],
      "id": "cc499baa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with torch.inference_mode():\n",
        "  BC_Model_V0.eval()\n",
        "  test_logits = BC_Model_V0(X_test).squeeze()\n",
        "  test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "  test_acc = accuracy_fn(y_true = y_test,\n",
        "                         y_pred = test_pred)\n",
        "print(f\"Test accuracy: {test_acc:.5f}%\")"
      ],
      "id": "a41cc7ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning and Architecture Adjustments\n",
        "\n",
        "To optimize the model’s performance, I experimented with different hyperparameters values:\n",
        "\n",
        "-   **Learning Rate** = **0.001** + **Neurons** = **8**\\\n",
        "    Training takes longer to reach the same accuracy (98.25%) and between 100 and 300 epochs it shows unstable behavior, a significant drop in accuracy occurs. This suggests that the model may experience optimization challenges - due to saddle point, poor local minima, momentum issues - before recovering.\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](Acc_N8_lr-0.001.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](Loss_N8_lr-0.001.png)\n",
        ":::\n",
        ":::::\n",
        "\n",
        "-   **Learning Rate** = **0.01** + **Neurons** = **9**\\\n",
        "    The model converges faster (due to a higher learning rate - 0.01), but around epoch 250, overfitting occurs, as validation accuracy stagnates ( and then proceedes to decline) while training accuracy continues increasing. This suggests that the model has learned the training data too well - including the noise - and generalizes worse on unseen data.\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](Acc_N9_lr-0.01.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](Loss_N9_lr-0.01.png)\n",
        ":::\n",
        ":::::\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This post highlights the applications of machine learning in medicine, demonstrating how deep learning can assist in early **breast cancer detection**. While this model is relatively simple, it serves as a great example of **binary classification**, where the goal is to distinguish between two categories: benign and malignant tumors.\n",
        "\n",
        "Even though the model achieves a high accuracy of **98.25%**, there are several ways to improve, such as:\\\n",
        "- **Hyperparameter tuning**: optimizing learning rates, batch sizes (if using a larger dataset) and architectures for better performance.\\\n",
        "- Using more complex models: implementing Convolutional Neural Networks (**CNNs**) for image-based cancer detection.\\\n",
        "- **Expanding the dataset**: including more diverse patient demographics to improve real-world generalization.\n",
        "\n",
        "As machine learning continues to evolve, models like this can contribute to early disease detection, improving patient outcomes and supporting medical professionals in their decision-making.\n",
        "\n",
        "### Links\n",
        "\n",
        "-   [PyTorch Official Documentation](https://pytorch.org/docs/stable/index.html)\n",
        "-   [Breast Cancer Wisconsin Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/data)\n",
        "-   [Learn PyTorch by Daniel Bourke](https://www.learnpytorch.io)\n",
        "\n",
        "## License\n",
        "\n",
        "[Cover image by Kjpargeter](https://www.freepik.com/author/kjpargeter)\n",
        "\n",
        "::: {.callout-tip collapse=\"true\" icon=\"file-code\"}\n",
        "### MIT License\n",
        "\n",
        "MIT License\\\n",
        "Copyright (c) 2021 Daniel Bourke\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        ":::"
      ],
      "id": "36c82217"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/johnarackal/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}