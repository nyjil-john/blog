{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Linear Regression and Nitrogen-16 Radioactive Decay\"\n",
        "author: \"Nyjil Arackal\"\n",
        "date: \"2025-01-26\"\n",
        "categories: [AI, Machine Learning, PyTorch, Physics]\n",
        "format: html\n",
        "image: \"post1_cover.jpg\"\n",
        "---\n",
        "\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Nitrogen-16 is a short-lived radioactive isotope that decays into Oxygen-16 through beta decay. It's commonly found in nuclear reactors and has a half-life of **7.13 seconds**.\n",
        "\n",
        "Radioactive decay follows an exponential curve, but we can apply logarithmic transformation to make it linear so that we can use **linear regression** to work with the data.\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"60%\"}\n",
        "### The Formula\n",
        "Exponential form: $A(t) = A_0 \\cdot e^{-\\lambda t}$\n",
        "\n",
        "After applying natural logarithm: $\\ln(A) = \\ln(A_0) - \\lambda t$\n",
        "\n",
        "We now have a linear relationship to work with.\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![](graph.png){width=\"100%\"}\n",
        ":::\n",
        ":::::\n",
        "\n",
        "Let's start with importing all the necessary libraries:\n"
      ],
      "id": "97e36ce3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "9fcc2f74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Translating the mathematical formula into Python code:\n"
      ],
      "id": "162a2873"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "half_life = 7.13\n",
        "decay_constant = torch.log(torch.tensor(2.0)) / half_life # Î» = ln(2) / half-life (by definition)\n",
        "time = torch.arange(0, 60, 1).unsqueeze(1) # let's pick a timeframe of 60 seconds\n",
        "A0 = 100 # initial activity (100%)\n",
        "\n",
        "radioactivity = A0 * torch.exp(-decay_constant * time) # exponential form\n",
        "log_radio = torch.log(radioactivity) # linear form (applied .log)"
      ],
      "id": "4b1d012a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating Data\n",
        "\n",
        "We have to split our data into training and testing sets: 80% for training and 20% for testing:\n"
      ],
      "id": "34b3a6ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_split = int(0.8 * len(time)) # we'll use 80% of data for training\n",
        "X_train, y_train = time[:train_split], log_radio[:train_split]\n",
        "X_test, y_test = time[train_split:], log_radio[train_split:]\n",
        "\n",
        "# first 48s of data to train, last 12s of data to test\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "id": "6a64ff0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a plotting function\n",
        "\n",
        "Let's create now a function that can plot on a graph all the data we are dealing with:\n"
      ],
      "id": "d46350a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plotting_fn(train_data = X_train,\n",
        "                train_labels = y_train,\n",
        "                test_data = X_test,\n",
        "                test_labels = y_test,\n",
        "                predictions = None):\n",
        " plt.figure(figsize=(10, 6))\n",
        "\n",
        " plt.scatter(train_data, train_labels, c = \"b\", s = 4, label = \"Training data\")\n",
        " plt.scatter(test_data, test_labels, c = \"r\", s = 4, label = \"Testing data\")\n",
        "\n",
        " if predictions is not None:\n",
        "   plt.scatter(test_data, predictions, c = \"g\", s = 4, label = \"Predictions\")\n",
        "\n",
        " plt.legend(prop={\"size\": 10});"
      ],
      "id": "47a54172",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see what the data look like:\n"
      ],
      "id": "7f43d70f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotting_fn();"
      ],
      "id": "7e110977",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Linear Regression Model\n",
        "\n",
        "We now have to create our model. Let's start with defining a class model:\n"
      ],
      "id": "33f7b494"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class LinearRegressionModel(nn.Module): # All PyTorch models are a subclass of nn.Module\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.weights = nn.Parameter(torch.randn(1, dtype = torch.float),\n",
        "                                requires_grad = True)\n",
        "    self.bias = nn.Parameter(torch.randn(1, dtype = torch.float),\n",
        "                            requires_grad = True)\n",
        "     # nn.Parameter transforms \"regular tensors\" into \"parameter tensors\" so that\n",
        "     # they can be tracked and updated\n",
        "  def forward(self, x):\n",
        "    return self.weights * x + self.bias\n",
        "  # the forward function defines what the model actually does, in this case it makes\n",
        "  # predictions using our formula y = (weights * x + bias)"
      ],
      "id": "fcaa44fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have created a class, now let's make a model out of it... let's call it Model_V0.\n",
        "\n",
        "### Initializing the model\n"
      ],
      "id": "6f96b2f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RANDOM_SEED = 42 # makes randomness less \"random\" (we can replicate the same\n",
        "# randomness over and over again)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "Model_V0 = LinearRegressionModel()\n",
        "\n",
        "list(Model_V0.parameters())\n",
        "# the model parameters (weight and bias) are now just random numbers, \n",
        "# that's because we've just created it and it has not been trained yet"
      ],
      "id": "1389084d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Starting point\n",
        "\n",
        "Let's look at the predictions of the untrained model:\n"
      ],
      "id": "fb3a930e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with torch.inference_mode():\n",
        "  y_preds = Model_V0(X_test)\n",
        "y_preds"
      ],
      "id": "30c6150f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotting_fn(predictions = y_preds)"
      ],
      "id": "a5073e02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the Loss Function and Optimizer\n"
      ],
      "id": "33f80c50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss_fn = nn.L1Loss() # calculates how bad the predictions are (L1Loss() uses Mean Absolute Error(MAE)) and calculates gradients\n",
        "optimizer = torch.optim.SGD(params = Model_V0.parameters(), lr = 0.01) # updates model's parameters based on SGD"
      ],
      "id": "f2b9c44c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n"
      ],
      "id": "fd16a547"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "epochs = 200\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Model_V0.train() # set the model in training mode\n",
        "  \n",
        "  y_pred = Model_V0(X_train) # training predictions\n",
        "  \n",
        "  loss = loss_fn(y_pred, y_train) # calculates the loss function in training\n",
        "  optimizer.zero_grad() # resets gradients to zero\n",
        "  loss.backward() # calculates the gradients of loss function with respect to each parameter\n",
        "  optimizer.step() # updates the parameters based on gradients\n",
        "  \n",
        "  Model_V0.eval() # evaluation mode\n",
        "  with torch.inference_mode():\n",
        "    test_pred = Model_V0(X_test)\n",
        "    test_loss = loss_fn(test_pred, y_test.type(torch.float))\n",
        "    \n",
        "  if epoch % 20 == 0:\n",
        "    print(f\"Epoch: {epoch} | Test Loss: {test_loss:.3f}\")\n"
      ],
      "id": "edf6fa48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing\n"
      ],
      "id": "c580e4b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Model_V0.eval()\n",
        "with torch.inference_mode():\n",
        "  final_pred = Model_V0(X_test)\n",
        "final_pred"
      ],
      "id": "15927f86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting final results\n",
        "\n",
        "Let's see what the final predictions are after training:\n"
      ],
      "id": "b4a2d82e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotting_fn(predictions = final_pred)"
      ],
      "id": "167e79ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "In this experiment, we successfully modeled the radioactive decay of Nitrogen-16 using linear regression. By applying a logarithmic transformation, we converted the exponential decay into a linear relationship, allowing us to predict future values using a simple model.\n",
        "\n",
        "While the model shows reasonable performance, we observed some fluctuations in the loss function (likely due to the logarithmic transformation). This suggests potential improvements.\n",
        "\n",
        "### Links\n",
        "\n",
        "-   [PyTorch Official Documentation](https://pytorch.org/docs/stable/index.html)\n",
        "-   [Radioactive Decay of Nitrogen-16](https://www.tpub.com/doenuclearphys/nuclearphysics20.htm)\n",
        "-   [Learn PyTorch by Daniel Bourke](https://www.learnpytorch.io)\n",
        "\n",
        "## License\n",
        "\n",
        "::: {.callout-tip collapse=\"true\" icon=\"file-code\"}\n",
        "### MIT License\n",
        "\n",
        "MIT License\\\n",
        "Copyright (c) 2021 Daniel Bourke\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the âSoftwareâ), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED âAS ISâ, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        ":::"
      ],
      "id": "18423d76"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/johnarackal/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}